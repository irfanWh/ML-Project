import streamlit as st
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# ========================================
# CONFIGURATION DE LA PAGE
# ========================================
st.set_page_config(
    page_title="D√©tection d'Anomalies Salariales",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========================================
# CHARGEMENT DES MOD√àLES ET MAPPINGS
# ========================================
@st.cache_resource
def load_models():
    """Charge tous les mod√®les et objets sauvegard√©s"""
    try:
        model = pickle.load(open('isolation_forest_model.pkl', 'rb'))
        scaler = pickle.load(open('scaler.pkl', 'rb'))
        features_list = pickle.load(open('features_list.pkl', 'rb'))
        pca = pickle.load(open('pca_model.pkl', 'rb'))
        return model, scaler, features_list, pca
    except Exception as e:
        st.error(f"Erreur de chargement des mod√®les : {e}")
        return None, None, None, None

@st.cache_data
def load_encoding_mappings():
    """Charge les mappings de d√©codage des colonnes cat√©gorielles"""
    try:
        mappings_df = pd.read_csv('prepareAdherents/encodage_mappings_adherents.csv')
        
        # Cr√©er des dictionnaires de d√©codage par colonne
        decode_maps = {}
        for col in mappings_df['column'].unique():
            col_data = mappings_df[mappings_df['column'] == col]
            decode_maps[col] = dict(zip(col_data['code'], col_data['original_value']))
        
        return decode_maps
    except Exception as e:
        st.warning(f"Mappings non charg√©s : {e}")
        return {}

def decode_dataframe(df, decode_maps):
    """D√©code les colonnes cat√©gorielles d'un dataframe"""
    df_decoded = df.copy()
    
    for col, mapping in decode_maps.items():
        if col in df_decoded.columns:
            df_decoded[col] = df_decoded[col].map(mapping).fillna(df_decoded[col])
    
    return df_decoded

model, scaler, features_list, pca = load_models()
decode_maps = load_encoding_mappings()

# ========================================
# SIDEBAR - NAVIGATION
# ========================================
st.sidebar.title("üîç D√©tection d'Anomalies")
st.sidebar.markdown("---")

page = st.sidebar.radio(
    "Navigation",
    ["üè† Accueil", "üìä Analyser Dataset", "üîé Pr√©dire Anomalie", "üìà Visualisations", "‚ÑπÔ∏è √Ä Propos"]
)

st.sidebar.markdown("---")
st.sidebar.info("**Mod√®le** : Isolation Forest\n\n**Features** : 13\n\n**Contamination** : 5%")

# ========================================
# PAGE 1 : ACCUEIL
# ========================================
if page == "üè† Accueil":
    st.title("üîç Syst√®me de D√©tection d'Anomalies Salariales")
    st.markdown("---")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ü§ñ Mod√®le", "Isolation Forest", "100 arbres")
    with col2:
        st.metric("üìä Features", "13", "S√©lectionn√©es")
    with col3:
        st.metric("üéØ Pr√©cision", "95%", "Contamination 5%")
    
    st.markdown("---")
    
    st.header("üìå Fonctionnalit√©s")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Analyser Dataset")
        st.write("""
        - Charger un fichier CSV d'entreprises
        - D√©tecter automatiquement les anomalies
        - T√©l√©charger les r√©sultats
        - Statistiques d√©taill√©es
        """)
        
        st.subheader("üîé Pr√©dire Anomalie")
        st.write("""
        - Saisir manuellement les donn√©es d'une entreprise
        - Obtenir une pr√©diction en temps r√©el
        - Score d'anomalie d√©taill√©
        """)
    
    with col2:
        st.subheader("üìà Visualisations")
        st.write("""
        - Distribution des scores d'anomalie
        - Analyse PCA en 2D
        - Comparaison par r√©gion
        - Graphiques interactifs
        """)
        
        st.subheader("‚ÑπÔ∏è √Ä Propos")
        st.write("""
        - Informations sur le mod√®le
        - Description des features
        - Documentation technique
        """)
    
    st.markdown("---")
    st.success("‚úÖ Mod√®les charg√©s avec succ√®s ! Utilisez le menu de navigation pour commencer.")

# ========================================
# PAGE 2 : ANALYSER DATASET
# ========================================
elif page == "üìä Analyser Dataset":
    st.title("üìä Analyse de Dataset Complet")
    st.markdown("---")
    
    uploaded_file = st.file_uploader("üìÅ Charger un fichier CSV", type=['csv'])
    
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        
        st.success(f"‚úÖ Fichier charg√© : {len(df)} entreprises")
        
        # V√©rifier les colonnes
        missing_features = [f for f in features_list if f not in df.columns]
        
        if missing_features:
            st.error(f"‚ùå Colonnes manquantes : {missing_features}")
        else:
            # Pr√©parer les donn√©es
            X = df[features_list].copy()
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
            
            # Normaliser
            X_scaled = scaler.transform(X)
            
            # Pr√©dire
            predictions = model.predict(X_scaled)
            scores = model.score_samples(X_scaled)
            
            # Ajouter au dataframe
            df['prediction'] = predictions
            df['score_anomalie'] = scores
            df['est_anomalie'] = (predictions == -1).astype(int)
            
            # Statistiques
            nb_anomalies = (df['est_anomalie'] == 1).sum()
            nb_normaux = (df['est_anomalie'] == 0).sum()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üö® Anomalies", nb_anomalies, f"{nb_anomalies/len(df)*100:.1f}%")
            with col2:
                st.metric("‚úÖ Normaux", nb_normaux, f"{nb_normaux/len(df)*100:.1f}%")
            with col3:
                st.metric("üìä Total", len(df))
            
            st.markdown("---")
            
            # Tableau des anomalies
            st.subheader("üö® Top 20 Entreprises Anormales")
            anomalies = df[df['est_anomalie'] == 1].nsmallest(20, 'score_anomalie')
            
            if len(anomalies) > 0:
                # D√©coder les colonnes cat√©gorielles
                anomalies_display = decode_dataframe(anomalies, decode_maps)
                
                cols_display = ['affiliateNumber', 'directionRegionale', 'nb_salaries', 'salaire_moyen', 
                               'masse_salariale_totale', 'taux_salaries_actifs', 'score_anomalie']
                cols_available = [c for c in cols_display if c in anomalies_display.columns]
                st.dataframe(anomalies_display[cols_available], use_container_width=True)
            else:
                st.info("Aucune anomalie d√©tect√©e.")
            
            st.markdown("---")
            
            # T√©l√©charger les r√©sultats
            col1, col2 = st.columns(2)
            
            with col1:
                # D√©coder avant t√©l√©chargement
                df_decoded = decode_dataframe(df, decode_maps)
                csv_all = df_decoded.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger R√©sultats Complets",
                    data=csv_all,
                    file_name="resultats_anomalies.csv",
                    mime="text/csv"
                )
            
            with col2:
                anomalies_decoded = decode_dataframe(df[df['est_anomalie'] == 1], decode_maps)
                csv_anomalies = anomalies_decoded.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger Anomalies Seulement",
                    data=csv_anomalies,
                    file_name="entreprises_anormales.csv",
                    mime="text/csv"
                )

# ========================================
# PAGE 3 : PR√âDIRE ANOMALIE
# ========================================
elif page == "üîé Pr√©dire Anomalie":
    st.title("üîé Pr√©dire une Anomalie")
    st.markdown("---")
    
    st.info("üí° Saisissez les informations d'une entreprise pour obtenir une pr√©diction")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìã Informations G√©n√©rales")
        
        # Dropdown pour Direction R√©gionale avec noms d√©cod√©s
        if 'directionRegionale' in decode_maps:
            region_options = decode_maps['directionRegionale']
            region_names = list(region_options.values())
            region_selected = st.selectbox("Direction R√©gionale", region_names, index=6 if len(region_names) > 6 else 0)
            # Trouver le code correspondant
            direction_regionale = [k for k, v in region_options.items() if v == region_selected][0]
        else:
            direction_regionale = st.number_input("Direction R√©gionale", min_value=0, max_value=20, value=6)
        
        nb_salaries = st.number_input("Nombre de Salari√©s", min_value=1, max_value=5000, value=50)
        masse_salariale = st.number_input("Masse Salariale Totale (‚Ç¨)", min_value=0.0, value=100000.0, step=1000.0)
        salaire_moyen = st.number_input("Salaire Moyen (‚Ç¨)", min_value=0.0, value=2000.0, step=100.0)
        salaire_median = st.number_input("Salaire M√©dian (‚Ç¨)", min_value=0.0, value=1800.0, step=100.0)
        salaire_max = st.number_input("Salaire Maximum (‚Ç¨)", min_value=0.0, value=3500.0, step=100.0)
        salaire_min = st.number_input("Salaire Minimum (‚Ç¨)", min_value=0.0, value=1500.0, step=100.0)
    
    with col2:
        st.subheader("üìä Statistiques")
        salaire_std = st.number_input("√âcart-type Salaire (‚Ç¨)", min_value=0.0, value=500.0, step=50.0)
        total_jours = st.number_input("Total Jours Travaill√©s", min_value=1, max_value=50000, value=1200)
        taux_actifs = st.slider("Taux Salari√©s Actifs (%)", min_value=0.0, max_value=100.0, value=95.0, step=1.0)
        salaire_par_jour = st.number_input("Salaire par Jour (‚Ç¨)", min_value=0.0, value=83.33, step=1.0)
        jours_par_salarie = st.number_input("Jours par Salari√©", min_value=1, max_value=365, value=24)
        ecart_region = st.number_input("√âcart Salaire R√©gion (‚Ç¨)", min_value=-10000.0, max_value=10000.0, value=0.0, step=100.0)
        compte_risque = st.selectbox("Compte √† Risque", [0, 1], format_func=lambda x: "Oui" if x == 1 else "Non")
    
    st.markdown("---")
    
    if st.button("üîç ANALYSER", type="primary", use_container_width=True):
        # Cr√©er le vecteur de features
        input_data = pd.DataFrame({
            'nb_salaries': [nb_salaries],
            'masse_salariale_totale': [masse_salariale],
            'salaire_moyen': [salaire_moyen],
            'salaire_median': [salaire_median],
            'salaire_max': [salaire_max],
            'salaire_min': [salaire_min],
            'salaire_std': [salaire_std],
            'total_jours_travailles': [total_jours],
            'taux_salaries_actifs': [taux_actifs],
            'salaire_par_jour': [salaire_par_jour],
            'jours_par_salarie': [jours_par_salarie],
            'ecart_salaire_region': [ecart_region],
            'compte_a_risque': [compte_risque]
        })
        
        # Normaliser
        input_scaled = scaler.transform(input_data)
        
        # Pr√©dire
        prediction = model.predict(input_scaled)[0]
        score = model.score_samples(input_scaled)[0]
        
        st.markdown("---")
        
        # R√©sultat
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if prediction == -1:
                st.error("üö® ANOMALIE D√âTECT√âE")
            else:
                st.success("‚úÖ ENTREPRISE NORMALE")
        
        with col2:
            st.metric("Score d'Anomalie", f"{score:.4f}")
        
        with col3:
            risk_level = "√âLEV√â" if score < -0.3 else "MOYEN" if score < -0.1 else "FAIBLE"
            color = "üî¥" if score < -0.3 else "üü†" if score < -0.1 else "üü¢"
            st.metric("Niveau de Risque", f"{color} {risk_level}")
        
        st.markdown("---")
        
        # Interpr√©tation
        st.subheader("üìã Interpr√©tation")
        
        # Afficher la r√©gion d√©cod√©e
        if 'directionRegionale' in decode_maps:
            region_name = decode_maps['directionRegionale'].get(direction_regionale, f"R√©gion {direction_regionale}")
            st.write(f"**R√©gion** : {region_name}")
        
        if prediction == -1:
            st.warning("""
            ‚ö†Ô∏è **Cette entreprise pr√©sente des caract√©ristiques anormales.**
            
            Recommandations :
            - V√©rifier la coh√©rence des donn√©es salariales
            - Analyser les √©carts par rapport √† la r√©gion
            - Investiguer si compte √† risque activ√©
            - Comparer avec des entreprises similaires
            """)
        else:
            st.info("""
            ‚úÖ **Cette entreprise pr√©sente un profil normal.**
            
            Les caract√©ristiques salariales sont coh√©rentes avec les entreprises similaires.
            """)

# ========================================
# PAGE 4 : VISUALISATIONS
# ========================================
elif page == "üìà Visualisations":
    st.title("üìà Visualisations et Analyses")
    st.markdown("---")
    
    # Charger le dataset complet
    try:
        df_viz = pd.read_csv('resultats_anomalies_detection.csv')
        
        tab1, tab2, tab3 = st.tabs(["üìä Distribution Scores", "üó∫Ô∏è PCA 2D", "üìç Analyse R√©gionale"])
        
        # TAB 1: Distribution
        with tab1:
            st.subheader("Distribution des Scores d'Anomalie")
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
            
            # Histogramme
            ax1.hist(df_viz['score_anomalie'], bins=50, color='skyblue', edgecolor='black')
            ax1.axvline(df_viz['score_anomalie'].mean(), color='red', linestyle='--', 
                       label=f"Moyenne: {df_viz['score_anomalie'].mean():.3f}")
            ax1.set_title('Distribution des Scores', fontweight='bold')
            ax1.set_xlabel('Score d\'anomalie')
            ax1.set_ylabel('Fr√©quence')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Boxplot
            normal = df_viz[df_viz['est_anomalie'] == 0]['score_anomalie']
            anomalie = df_viz[df_viz['est_anomalie'] == 1]['score_anomalie']
            ax2.boxplot([normal, anomalie], labels=['Normal', 'Anomalie'])
            ax2.set_title('Comparaison des Scores', fontweight='bold')
            ax2.set_ylabel('Score d\'anomalie')
            ax2.grid(True, alpha=0.3)
            
            st.pyplot(fig)
        
        # TAB 2: PCA
        with tab2:
            st.subheader("Visualisation PCA 2D")
            
            X_viz = df_viz[features_list].copy()
            X_viz = X_viz.replace([np.inf, -np.inf], np.nan).fillna(X_viz.median())
            X_scaled_viz = scaler.transform(X_viz)
            X_pca_viz = pca.transform(X_scaled_viz)
            
            fig, ax = plt.subplots(figsize=(12, 8))
            
            colors = ['blue' if x == 0 else 'red' for x in df_viz['est_anomalie']]
            ax.scatter(X_pca_viz[:, 0], X_pca_viz[:, 1], c=colors, alpha=0.5, s=30)
            
            ax.set_title('D√©tection d\'Anomalies - PCA 2D', fontsize=16, fontweight='bold')
            ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)
            ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)
            ax.legend(['Normal', 'Anomalie'])
            ax.grid(True, alpha=0.3)
            
            st.pyplot(fig)
        
        # TAB 3: R√©gions
        with tab3:
            st.subheader("Analyse par R√©gion")
            
            if 'directionRegionale' in df_viz.columns:
                # D√©coder les r√©gions
                df_viz_decoded = decode_dataframe(df_viz, decode_maps)
                
                region_stats = df_viz_decoded.groupby('directionRegionale').agg({
                    'est_anomalie': ['sum', 'mean', 'count']
                }).round(3)
                
                region_stats.columns = ['Nb_Anomalies', 'Taux_Anomalie', 'Total']
                region_stats = region_stats.sort_values('Nb_Anomalies', ascending=False)
                
                st.dataframe(region_stats, use_container_width=True)
                
                fig, ax = plt.subplots(figsize=(12, 6))
                region_stats['Nb_Anomalies'].plot(kind='bar', color='coral', edgecolor='black', ax=ax)
                ax.set_title('Nombre d\'Anomalies par R√©gion', fontsize=14, fontweight='bold')
                ax.set_xlabel('Direction R√©gionale')
                ax.set_ylabel('Nombre d\'Anomalies')
                ax.grid(True, alpha=0.3, axis='y')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                st.pyplot(fig)
            else:
                st.warning("Colonne 'directionRegionale' non trouv√©e dans le dataset.")
    
    except FileNotFoundError:
        st.error("‚ùå Fichier 'resultats_anomalies_detection.csv' introuvable. Veuillez d'abord analyser un dataset.")

# ========================================
# PAGE 5 : √Ä PROPOS
# ========================================
elif page == "‚ÑπÔ∏è √Ä Propos":
    st.title("‚ÑπÔ∏è √Ä Propos du Syst√®me")
    st.markdown("---")
    
    st.header("ü§ñ Mod√®le : Isolation Forest")
    st.write("""
    L'**Isolation Forest** est un algorithme de d√©tection d'anomalies non supervis√© qui :
    - Isole les observations en s√©lectionnant al√©atoirement une feature
    - S√©pare les valeurs entre le min et max de cette feature
    - Les anomalies n√©cessitent moins de partitions (sont isol√©es plus rapidement)
    - Score n√©gatif = plus l'entreprise est anormale
    """)
    
    st.markdown("---")
    
    st.header("üìä Features Utilis√©es (13)")
    
    features_info = {
        "nb_salaries": "Nombre de salari√©s dans l'entreprise",
        "masse_salariale_totale": "Somme totale des salaires (‚Ç¨)",
        "salaire_moyen": "Salaire moyen des employ√©s (‚Ç¨)",
        "salaire_median": "Salaire m√©dian (‚Ç¨)",
        "salaire_max": "Salaire maximum (‚Ç¨)",
        "salaire_min": "Salaire minimum (‚Ç¨)",
        "salaire_std": "√âcart-type des salaires (‚Ç¨)",
        "total_jours_travailles": "Total de jours travaill√©s",
        "taux_salaries_actifs": "Pourcentage de salari√©s actifs (%)",
        "salaire_par_jour": "Co√ªt salarial quotidien (‚Ç¨/jour)",
        "jours_par_salarie": "Moyenne de jours par salari√©",
        "ecart_salaire_region": "Diff√©rence par rapport √† la moyenne r√©gionale (‚Ç¨)",
        "compte_a_risque": "Indicateur de compte bancaire √† risque (0/1)"
    }
    
    for feature, description in features_info.items():
        st.write(f"**{feature}** : {description}")
    
    st.markdown("---")
    
    st.header("‚öôÔ∏è Param√®tres du Mod√®le")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**n_estimators** : 100 arbres")
        st.write("**contamination** : 0.05 (5% anomalies)")
        st.write("**random_state** : 42")
    
    with col2:
        st.write("**Normalisation** : StandardScaler")
        st.write("**PCA** : 2 composantes principales")
        st.write("**Score seuil** : -0.1 (personnalisable)")
    
    st.markdown("---")
    
    st.success("‚úÖ Syst√®me d√©velopp√© pour d√©tecter les anomalies dans les donn√©es salariales des entreprises.")
